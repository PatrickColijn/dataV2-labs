{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inbalanced Classes\n",
    "## In this lab, we are going to explore a case of imbalanced classes. \n",
    "\n",
    "\n",
    "Like we disussed in class, when we have noisy data, if we are not careful, we can end up fitting our model to the noise in the data and not the 'signal'-- the factors that actually determine the outcome. This is called overfitting, and results in good results in training, and in bad results when the model is applied to real data. Similarly, we could have a model that is too simplistic to accurately model the signal. This produces a model that doesnt work well (ever). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, download the data from: https://www.kaggle.com/ntnu-testimon/paysim1. Import the dataset and provide some discriptive statistics and plots. What do you think will be the important features in determining the outcome?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load paysim1 data into dataframe\n",
    "sim = pd.read_csv(#data not include due size)\n",
    "\n",
    "# Print descriptive data and overview\n",
    "print(sim.dtypes)\n",
    "sim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataframe descriptions\n",
    "sim.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the distribution of the outcome? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency distribution by using a histogram \n",
    "sim[\"isFraud\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the dataset. How are you going to integrate the time variable? Do you think the step (integer) coding in which it is given is appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency distribution by using a histogram \n",
    "sim[\"step\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a logisitc regression classifier and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused columns \n",
    "sim = sim.drop([\"nameOrig\",\"nameDest\"],axis=1)\n",
    "\n",
    "# Turn data into dummies\n",
    "sim = pd.get_dummies(sim,columns=[\"type\"])\n",
    "\n",
    "# Print overview \n",
    "print(sim.shape)\n",
    "sim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define independent variables and dependent variables\n",
    "x = sim.drop([\"isFraud\",\"isFlaggedFraud\"],axis=1)\n",
    "y = sim[\"isFraud\"]\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now pick a model of your choice and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "\n",
    "# Define independent variables and dependent variables\n",
    "x = sim.drop([\"isFraud\",\"isFlaggedFraud\"],axis=1)\n",
    "y = sim[\"isFraud\"]\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Initialize Random Forest Model\n",
    "RFC = RandomForestClassifier(n_estimators=100, bootstrap = True,max_features = 'sqrt')\n",
    "\n",
    "# Train the model\n",
    "RFC.fit(x_train, y_train)\n",
    "\n",
    "# Show predicitons\n",
    "y_pred_RFC = RFC.predict(x_test)\n",
    "\n",
    "# Create classification report\n",
    "classification_report(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(classification_report(y_test, y_pred_RFC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which model worked better and how do you know?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your response here\n",
    "'''\n",
    "Based on average the second model is better\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
